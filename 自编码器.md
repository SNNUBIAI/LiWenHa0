# 自编码器

- AE算法原理

  - **输出层的神经元数量往往与输入层的神经元数量一致**
  - **网络架构往往呈对称性，且中间结构简单、两边结构复杂**

  无监督学习模型，将输入数据X作为监督，知道神经网络学习一个映射关系呃，从而得到一个重构输出X`.

  算法模型包含两个主要的部分：Encoder（编码器）和Decoder（解码器）

  编码器将高维输入X编码成低维的隐变量h从而强迫神经网络学习有信息量的特征；解码器将隐藏层的变量h还原到初始维度。最好的状态就是解码器的输出能够完美地或者近似恢复出原来的输入。

![img](https://pic2.zhimg.com/80/v2-8f16ee4ca80f1d5057d1b5c656c94a61_720w.webp)

- 自编码可以实现类似于PCA等数据降维、数据压缩的特性。从上面自编码的网络结构图，如果输入层神经元的个数n大于隐层神经元个数m，那么我们就相当于把数据从n维降到了m维；然后我们利用这m维的特征向量，进行重构原始的数据。这个跟PCA降维一模一样，只不过PCA是通过求解特征向量，进行降维，是一种线性的降维方式，而自编码是一种非线性降维。

  自编码的目标便是在迭代过程中不断优化损失函数（重构误差）

## 稀疏自编码（SAE）

- A、网络各层参数预训练。我们在以前的神经网络中，参数的初始化都是用随机初始化方法，然而这种方法，对于深层网络，在低层中，参数很难被训练，于是Hinton提出了参数预训练，这个主要就是采用RBM、以及我们接下来要讲的自编码，对网络的每一层进行参数初始化。也就是我们这边要学的稀疏自编码就是为了对网络的每一层进行参数初始化，仅仅是为了获得初始的参数值而已（这就是所谓的无监督参数初始化，或者称之为“无监督 pre-training”）。

  B、比如采用自编码，我们可以把网络从第一层开始自编码训练，在每一层学习到的隐藏特征表示后作为下一层的输入，然后下一层再进行自编码训练，对每层网络的进行逐层无监督训练。

  C、当我们无监督训练完毕后，我们要用于某些指定的任务，比如分类，这个时候我们可以用有标签的数据对整个网络的参数继续进行梯度下降调整。

  这就是深层网络的训练思想，总体归结为：无监督预训练、有监督微调。

  - **稀疏自编码**仅仅只是为了获得参数的初始值而已。栈式自编码神经网络是一个由多层稀疏自编码器组成的神经网络，其前一层自编码器的输出作为其后一层自编码器的输入。
  
    
  
    
  
    

## 卷积自编码器

- 编码器：使用卷积层将图像映射到一个低维潜在空间，提取图像特征
- 解码器：使用反卷积层将潜在空间的特征还原成图像，重构该图像，是的重建图像尽可能接近原始图像

## 降噪自编码器（DAE）

- 引入噪声，使得图片的编码区域得到扩大，从而掩盖掉失真的空白编码点。
- 去噪自动编码器将噪声图像作为输入，输出层的目标是没有噪声的原始输入。
- 自编码器的编码空间包含更强大的信息，可以重建图像。换言之就是，添加到输入中的噪声充当正则化器。

# 变分自编码器（VAE)

![img](https://www.gwylab.com/files/VAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.files/image011.jpg)

- 在auto-encoder中，编码器是直接产生一个编码的，但是在VAE中，为了给编码添加合适的噪音，编码器会输出两个编码，一个是原有编码(![img](https://www.gwylab.com/files/VAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.files/image012.png))，另外一个是控制噪音干扰程度的编码(![img](https://www.gwylab.com/files/VAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.files/image013.png))
- 损失函数方面除了重构损失外面，还有基于施加噪声方面的损失函数即将(![img](https://www.gwylab.com/files/VAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.files/image013.png))赋为接近负无穷大的值就好了，exp(![img](https://www.gwylab.com/files/VAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.files/image015.png))-(1+![img](https://www.gwylab.com/files/VAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.files/image015.png))在![img](https://www.gwylab.com/files/VAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.files/image015.png)=0处取得最小值，于是(![img](https://www.gwylab.com/files/VAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.files/image013.png))就会避免被赋值为负无穷大。
- VAE中我们考虑用高数分布的参数作为编码值实现编码
- ![img](https://www.gwylab.com/files/VAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.files/image018.jpg)
-  如上图所示，m代表着编码维度上的编号，譬如实现一个512维的编码，m的取值范围就是1,2,3……512。m会服从于一个概率分布P(m)（多项式分布）。现在编码的对应关系是，每采样一个m，其对应到一个小的高斯分布![img](https://www.gwylab.com/files/VAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.files/image019.png)，P(X)就可以等价为所有的这些高斯分布的叠加，即

 ![img](https://www.gwylab.com/files/VAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.files/image020.png)

- 编码器需要输出该类高斯分布的均值  与标准差  作为编码器的输出。

- 编码器需要输出该类高斯分布的均值  与标准差  作为编码器的输出。

- 从构建的高斯分布中随机采样出**一个数值** � ，将该数值输入解码器。
- 解码器基于 � 进行解码，并最终输出与样本的原始特征结构一致的数据，作为VAE的输出 �′ 。

![img](https://pic2.zhimg.com/80/v2-2f05afe9d97ac069bbcb0c3d50dbd441_720w.webp)

![img](https://pic2.zhimg.com/80/v2-76cc6a3f913f24d9126204857616aab5_720w.webp)

![img](https://pic3.zhimg.com/80/v2-92b74d611f1cdf520c42360ed98f4632_720w.webp)

任意样本经过Encoder后会输出一个均值 与一个标准差 ，们可以从每个正态分布中随机采样出一个样本，并按样本排列顺序拼凑在一起，构成形如(m,1)的 z向量。此时， z向量再输入Decoder，Decoder的输入层就只能有1个神经元，因为z 只有一列。但是，**当前的均值和标准差不是真实数据的统计量，而是通过Encoder推断出的、当前样本数据可能服从的任意分布中的属性**。

例如，我们可以令Encoder的输出层存在3个神经元，这样Encoder就会对每一个样本推断出三对不同的均值和标准差。这个行为相当于对样本数据所属的原始分布进行估计，但给出了三个可能的答案。因此现在，在每个样本下，我们就可以基于三个均值和标准差的组合生成三个不同的正态分布了。

![img](https://pic1.zhimg.com/80/v2-69f760af1ae9e9bc6550e784349dd574_720w.webp)

![img](https://pic2.zhimg.com/80/v2-3fdc71cb7b0a82a345f9f654ae52c355_720w.webp)

每个样本对应了3个正态分布，而3个正态分布中可以分别抽取出三个数字z，此时每个隐式表示z就是一个形如(m,3)的矩阵。将这一矩阵放入Decoder，则Decoder的输入层也需要有三个神经元。此时，我们的隐式空间就是(m,3)。
